{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d2b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58752de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[[1., 2., 3.],\n",
      "          [4., 5., 6.]]]])\n",
      "Input Shape: torch.Size([1, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch는 4차원 Input만을 지원\n",
    "test_input = torch.Tensor([[[[1,2,3],[4,5,6]]]])\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Input Shape:\", test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684f4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sample, self).__init__()\n",
    "        self.layer = nn.ConvTranspose2d(1, 3, kernel_size=4, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a35c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92234d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: layer.weight\n",
      "Param: tensor([[[[-0.0015,  0.0079, -0.0088,  0.0723],\n",
      "          [ 0.0662, -0.0351, -0.1160,  0.0344],\n",
      "          [-0.1198, -0.0460,  0.0185,  0.0555],\n",
      "          [-0.1271, -0.0537, -0.0528,  0.0959]],\n",
      "\n",
      "         [[ 0.1211, -0.1405, -0.0044, -0.0523],\n",
      "          [-0.0445, -0.0861,  0.0675, -0.1312],\n",
      "          [ 0.1194,  0.0249,  0.0301, -0.0980],\n",
      "          [ 0.1315,  0.0350,  0.1157,  0.0930]],\n",
      "\n",
      "         [[ 0.0998, -0.0470,  0.1280, -0.0012],\n",
      "          [ 0.0793, -0.1085,  0.0375, -0.0493],\n",
      "          [-0.0272, -0.0776, -0.0518, -0.1184],\n",
      "          [-0.0899,  0.0993, -0.0183,  0.0828]]]])\n",
      "Param Shape: torch.Size([1, 3, 4, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.state_dict().items():\n",
    "    print(\"Name:\", name)\n",
    "    print(\"Param:\", param) # pytorch의 기본 초기화 방법에 따라 weight값 결정\n",
    "    print(\"Param Shape:\", param.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d7aa5",
   "metadata": {},
   "source": [
    "#### 직관적이고 Simple한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "622d847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_weight = torch.Tensor(np.linspace(0.1, 4.8, 48))\n",
    "_weight = _weight.view(1, 3, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf8f2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch, channel, width, height = net.layer.weight.shape\n",
    "    \n",
    "    for b in range(batch):\n",
    "        for c in range(channel):\n",
    "            for w in range(width):\n",
    "                for h in range(height):\n",
    "                    net.layer.weight[b][c][w][h] = _weight[b][c][w][h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0a51c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Transpose Convolution Model Weight==\n",
      "Parameter containing:\n",
      "tensor([[[[0.1000, 0.2000, 0.3000, 0.4000],\n",
      "          [0.5000, 0.6000, 0.7000, 0.8000],\n",
      "          [0.9000, 1.0000, 1.1000, 1.2000],\n",
      "          [1.3000, 1.4000, 1.5000, 1.6000]],\n",
      "\n",
      "         [[1.7000, 1.8000, 1.9000, 2.0000],\n",
      "          [2.1000, 2.2000, 2.3000, 2.4000],\n",
      "          [2.5000, 2.6000, 2.7000, 2.8000],\n",
      "          [2.9000, 3.0000, 3.1000, 3.2000]],\n",
      "\n",
      "         [[3.3000, 3.4000, 3.5000, 3.6000],\n",
      "          [3.7000, 3.8000, 3.9000, 4.0000],\n",
      "          [4.1000, 4.2000, 4.3000, 4.4000],\n",
      "          [4.5000, 4.6000, 4.7000, 4.8000]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"==Transpose Convolution Model Weight==\")\n",
    "print(net.layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9aaf2204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[[[ 0.1000,  0.4000,  1.0000,  1.6000,  1.7000,  1.2000],\n",
      "          [ 0.9000,  2.9000,  6.2000,  8.3000,  7.5000,  4.8000],\n",
      "          [ 2.9000,  7.7000, 14.6000, 16.7000, 13.9000,  8.4000],\n",
      "          [ 4.9000, 12.5000, 23.0000, 25.1000, 20.3000, 12.0000],\n",
      "          [ 5.2000, 12.1000, 20.8000, 22.3000, 17.0000,  9.6000]],\n",
      "\n",
      "         [[ 1.7000,  5.2000, 10.6000, 11.2000,  9.7000,  6.0000],\n",
      "          [ 8.9000, 22.1000, 39.8000, 41.9000, 33.1000, 19.2000],\n",
      "          [10.9000, 26.9000, 48.2000, 50.3000, 39.5000, 22.8000],\n",
      "          [12.9000, 31.7000, 56.6000, 58.7000, 45.9000, 26.4000],\n",
      "          [11.6000, 26.5000, 44.8000, 46.3000, 34.6000, 19.2000]],\n",
      "\n",
      "         [[ 3.3000, 10.0000, 20.2000, 20.8000, 17.7000, 10.8000],\n",
      "          [16.9000, 41.3000, 73.4000, 75.5000, 58.7000, 33.6000],\n",
      "          [18.9000, 46.1000, 81.8000, 83.9000, 65.1000, 37.2000],\n",
      "          [20.9000, 50.9000, 90.2000, 92.3000, 71.5000, 40.8000],\n",
      "          [18.0000, 40.9000, 68.8000, 70.3000, 52.2000, 28.8000]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "Output Shape: torch.Size([1, 3, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "output = net(test_input)\n",
    "print(\"Output:\", output)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02eb3bb",
   "metadata": {},
   "source": [
    "## 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082ab3f",
   "metadata": {},
   "source": [
    "torch.nn.convTranspose2d 메소드가 작동하는 방식\n",
    "1. nn.convTranspose2d를 만들 때 설정한 input channel수와 output channel수, kernel_size를 고려하여 weight를 만든다\n",
    "2. input의 element별로 filter와 곱해준다. stride만큼 이동해주면서 만들어준다.\n",
    "3. 나온 모든 결과값을 element-wise 하게 더해서 최종결과를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe3c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
