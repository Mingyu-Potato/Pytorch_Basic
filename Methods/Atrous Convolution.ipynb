{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3cee06b",
   "metadata": {},
   "source": [
    "### Atrous Convolution 장점\n",
    "---\n",
    "- receptive field가 커지면서 파라미터 갯수가 줄어들기 때문에 <b>연산량 관점에서 효과</b>를 볼 수 있다.\n",
    "- 필터가 넓은 범위가 봄으로써, <b>다양한 scale의 receptive field</b>를 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcd14c",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3d405",
   "metadata": {},
   "source": [
    "### Atrous Convolution외에 넓은 receptive field를 보는 방법이 있다.\n",
    "### -> Pooling을 통하여 Input을 Downsampling</b>\n",
    "---\n",
    "<span style=\"font-size: 18px;\">classification과 detection</span> 둘 다 <b>object의 존재 여부</b>가 목적이다.<br>\n",
    "두 방법 모두 여러 단계의 convolution과 pooling 연산을 거쳐서 이미지 내의 <b>object의 핵심이 되는 feature를 추출</b>해야 한다. 따라서 위치를 파악하기 위해 핵심이 되는 feature를 max pooling을 통해 뽑아내게 되고 그 결과, feature map의 size는 줄어든다.<br>\n",
    "위 방법은 object의 위치를 찾는 데 좋은 효과를 볼 수 있다. 즉, 픽셀 단위의 detail 보다는 <b>object의 위치를 찾는 global</b>을 택한 것이다.<br><br>\n",
    "\n",
    "또한 위 방법은, <b>detail에 취약하기 때문에 Segmentation에는 약점</b>을 보일 수 있다.<br>\n",
    "따라서, detail을 잃지 않고 넓은 범위를 receptive field로 할 수 있는 <b>Atrous Convolution을 이용하여 Segmentation</b>을 하는 것은 상당한 효과가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a263036",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c747dc",
   "metadata": {},
   "source": [
    "### DeepLab v1은 dilation이 단순하게 12로 고정\n",
    "\n",
    "### DeepLab v2에서는 multi-scale에 더 잘 대응할 수 있도록 dilation을 (6, 12, 18, 24)를 적용해서 합쳐서 사용\n",
    "- dilation이 커질수록 receptive field가 커지는 장점은 있지만, 유효한 weight 수는 줄어든다.(zero-padding이 많아짐)\n",
    "- ASPP-S(dilation=(2,4,8,12))와 ASPP-L(dilation=(6,12,18,24))를 이용했을 때, ASPP-L > ASPP-S > LargeFOV 순이었다.\n",
    "- 넓은 multi-scale의 receptive field를 적용해주는 것이 더 좋은 결과를 기대할 수 있다.\n",
    "\n",
    "### DeepLab v3에서는 output stride라는 개념 도입\n",
    "- output stride는 입력 이미지의 h, w가 feature map의 h, w보다 몇 배 큰지를 의미한다.\n",
    "- DeepLab v2에서 batch normalization이 각 conv에 추가되었고, output stride에 따라 dilation이 다르다.\n",
    "- output stride가 16인 경우에는 dilation=(6,12,18), output stride가 8인 경우에는 dilation=(12,24,36)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bbb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eb8e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atrous Spatial Pyramid Pooling (DeepLab v3)\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_classes: int = 1, dilation: int = 6):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation*2, dilation=dilation*2)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation*3, dilation=dilation*3)\n",
    "        self.bn4 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_pool = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn_pool = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv_cat = nn.Conv2d(out_channels*5, out_channels, kernel_size=1)\n",
    "        self.bn_cat = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(out_channels, n_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_h = x.size()[2]\n",
    "        x_w = x.size()[3]\n",
    "\n",
    "        out1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        out2 = F.relu(self.bn2(self.conv2(x)))\n",
    "        out3 = F.relu(self.bn3(self.conv3(x)))\n",
    "        out4 = F.relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        out5 = F.relu(self.bn_pool(self.conv_pool(self.avg_pool(x))))\n",
    "        out5 = F.upsample(out5, size=(x_h, x_w), mode='bilinear')\n",
    "\n",
    "        out_cat = torch.cat([out1, out2, out3, out4, out5], 1)\n",
    "        out_cat = F.relu(self.bn_cat(self.conv_cat(out_cat)))\n",
    "        out = self.conv_out(out_cat)\n",
    "        \n",
    "        return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f124975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "sample_data = torch.randn(4, 128, 224, 224)\n",
    "\n",
    "# model\n",
    "aspp1 = ASPP(128, 256, n_classes=1)\n",
    "\n",
    "# result\n",
    "pred = aspp1(sample_data)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3845871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
